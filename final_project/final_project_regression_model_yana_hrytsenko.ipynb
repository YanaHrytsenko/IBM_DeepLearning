{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning Final project\n",
    "#Yana Hrytsenko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  Strength  \n",
       "0            1040.0           676.0   28     79.99  \n",
       "1            1055.0           676.0   28     61.89  \n",
       "2             932.0           594.0  270     40.27  \n",
       "3             932.0           594.0  365     41.05  \n",
       "4             978.4           825.5  360     44.30  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Download data\n",
    "concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\n",
    "concrete_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check shape of the dataset\n",
    "concrete_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>281.167864</td>\n",
       "      <td>73.895825</td>\n",
       "      <td>54.188350</td>\n",
       "      <td>181.567282</td>\n",
       "      <td>6.204660</td>\n",
       "      <td>972.918932</td>\n",
       "      <td>773.580485</td>\n",
       "      <td>45.662136</td>\n",
       "      <td>35.817961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.506364</td>\n",
       "      <td>86.279342</td>\n",
       "      <td>63.997004</td>\n",
       "      <td>21.354219</td>\n",
       "      <td>5.973841</td>\n",
       "      <td>77.753954</td>\n",
       "      <td>80.175980</td>\n",
       "      <td>63.169912</td>\n",
       "      <td>16.705742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>192.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>730.950000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>272.900000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>779.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>34.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>142.950000</td>\n",
       "      <td>118.300000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1029.400000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>46.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>540.000000</td>\n",
       "      <td>359.400000</td>\n",
       "      <td>200.100000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>992.600000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>82.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Cement  Blast Furnace Slag      Fly Ash        Water  \\\n",
       "count  1030.000000         1030.000000  1030.000000  1030.000000   \n",
       "mean    281.167864           73.895825    54.188350   181.567282   \n",
       "std     104.506364           86.279342    63.997004    21.354219   \n",
       "min     102.000000            0.000000     0.000000   121.800000   \n",
       "25%     192.375000            0.000000     0.000000   164.900000   \n",
       "50%     272.900000           22.000000     0.000000   185.000000   \n",
       "75%     350.000000          142.950000   118.300000   192.000000   \n",
       "max     540.000000          359.400000   200.100000   247.000000   \n",
       "\n",
       "       Superplasticizer  Coarse Aggregate  Fine Aggregate          Age  \\\n",
       "count       1030.000000       1030.000000     1030.000000  1030.000000   \n",
       "mean           6.204660        972.918932      773.580485    45.662136   \n",
       "std            5.973841         77.753954       80.175980    63.169912   \n",
       "min            0.000000        801.000000      594.000000     1.000000   \n",
       "25%            0.000000        932.000000      730.950000     7.000000   \n",
       "50%            6.400000        968.000000      779.500000    28.000000   \n",
       "75%           10.200000       1029.400000      824.000000    56.000000   \n",
       "max           32.200000       1145.000000      992.600000   365.000000   \n",
       "\n",
       "          Strength  \n",
       "count  1030.000000  \n",
       "mean     35.817961  \n",
       "std      16.705742  \n",
       "min       2.330000  \n",
       "25%      23.710000  \n",
       "50%      34.445000  \n",
       "75%      46.135000  \n",
       "max      82.600000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#describe dataset\n",
    "concrete_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cement                0\n",
       "Blast Furnace Slag    0\n",
       "Fly Ash               0\n",
       "Water                 0\n",
       "Superplasticizer      0\n",
       "Coarse Aggregate      0\n",
       "Fine Aggregate        0\n",
       "Age                   0\n",
       "Strength              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how many NULL values are present in our dataset\n",
    "concrete_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#Part A: Build a baseline model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a regression model\n",
    "def regression_model(n_cols, hidden_n_nodes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_n_nodes, activation='relu', input_shape=(n_cols,))) #one hidden layer with 10 nodes\n",
    "    model.add(Dense(1)) \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Randomly split the data into a training and test sets\n",
    "\n",
    "#Split the dataset into predictors and target\n",
    "concrete_data_columns = concrete_data.columns\n",
    "\n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\n",
    "target = concrete_data['Strength'] # Strength column\n",
    "\n",
    "n_cols = predictors.shape[1] # number of predictors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 246477.4253 - val_loss: 158727.3084\n",
      "Epoch 2/10\n",
      " - 0s - loss: 121469.1070 - val_loss: 71411.0409\n",
      "Epoch 3/10\n",
      " - 0s - loss: 52572.4183 - val_loss: 28526.4978\n",
      "Epoch 4/10\n",
      " - 0s - loss: 22000.8113 - val_loss: 12033.5577\n",
      "Epoch 5/10\n",
      " - 0s - loss: 11572.3380 - val_loss: 7707.0007\n",
      "Epoch 6/10\n",
      " - 0s - loss: 9065.8346 - val_loss: 6899.7896\n",
      "Epoch 7/10\n",
      " - 0s - loss: 8613.8624 - val_loss: 6743.3019\n",
      "Epoch 8/10\n",
      " - 0s - loss: 8390.0586 - val_loss: 6635.4738\n",
      "Epoch 9/10\n",
      " - 0s - loss: 8192.7419 - val_loss: 6526.5842\n",
      "Epoch 10/10\n",
      " - 0s - loss: 7995.5614 - val_loss: 6417.0221\n",
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 7789.5990 - val_loss: 6314.2659\n",
      "Epoch 2/10\n",
      " - 0s - loss: 7598.6791 - val_loss: 6202.3534\n",
      "Epoch 3/10\n",
      " - 0s - loss: 7394.8609 - val_loss: 6095.5268\n",
      "Epoch 4/10\n",
      " - 0s - loss: 7197.4323 - val_loss: 5987.2161\n",
      "Epoch 5/10\n",
      " - 0s - loss: 6994.4329 - val_loss: 5889.8754\n",
      "Epoch 6/10\n",
      " - 0s - loss: 6812.7433 - val_loss: 5787.7541\n",
      "Epoch 7/10\n",
      " - 0s - loss: 6619.2773 - val_loss: 5701.6967\n",
      "Epoch 8/10\n",
      " - 0s - loss: 6428.4198 - val_loss: 5597.1697\n",
      "Epoch 9/10\n",
      " - 0s - loss: 6255.0906 - val_loss: 5508.3198\n",
      "Epoch 10/10\n",
      " - 0s - loss: 6065.3901 - val_loss: 5423.2745\n",
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 5891.0746 - val_loss: 5339.4470\n",
      "Epoch 2/10\n",
      " - 0s - loss: 5722.8066 - val_loss: 5255.1841\n",
      "Epoch 3/10\n",
      " - 0s - loss: 5546.5950 - val_loss: 5172.9249\n",
      "Epoch 4/10\n",
      " - 0s - loss: 5382.8400 - val_loss: 5089.1547\n",
      "Epoch 5/10\n",
      " - 0s - loss: 5220.6347 - val_loss: 5014.6285\n",
      "Epoch 6/10\n",
      " - 0s - loss: 5059.8592 - val_loss: 4935.4808\n",
      "Epoch 7/10\n",
      " - 0s - loss: 4907.5623 - val_loss: 4862.0570\n",
      "Epoch 8/10\n",
      " - 0s - loss: 4751.1009 - val_loss: 4790.6281\n",
      "Epoch 9/10\n",
      " - 0s - loss: 4603.6531 - val_loss: 4709.9304\n",
      "Epoch 10/10\n",
      " - 0s - loss: 4453.6925 - val_loss: 4644.1261\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "\n",
    "hidden_n_nodes = 10\n",
    "\n",
    "model = regression_model(n_cols, hidden_n_nodes)\n",
    "\n",
    "list_of_scores = []\n",
    "\n",
    "num_runs = 50\n",
    "for i in range (0,num_runs):\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(predictors, target, validation_split=0.3, epochs=50, verbose=2) \n",
    "\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(predictors, target, verbose=0)\n",
    "    \n",
    "    list_of_scores.append(scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean is:  5901.6605544599515\n",
      "std_dev is:  1495.1246884426498\n"
     ]
    }
   ],
   "source": [
    "#5. Report the mean and the standard deviation of the mean squared errors\n",
    "\n",
    "from statistics import mean, stdev\n",
    "\n",
    "mean_score = mean(list_of_scores)\n",
    "\n",
    "stand_dev = stdev(list_of_scores)\n",
    "\n",
    "print(\"mean is: \", mean_score)\n",
    "\n",
    "print(\"std_dev is: \", stand_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 153.1666 - val_loss: 115.1574\n",
      "Epoch 2/10\n",
      " - 0s - loss: 152.8141 - val_loss: 114.7841\n",
      "Epoch 3/10\n",
      " - 0s - loss: 152.6054 - val_loss: 114.3723\n",
      "Epoch 4/10\n",
      " - 0s - loss: 152.2855 - val_loss: 113.5694\n",
      "Epoch 5/10\n",
      " - 0s - loss: 152.0202 - val_loss: 113.6587\n",
      "Epoch 6/10\n",
      " - 0s - loss: 151.7934 - val_loss: 112.7540\n",
      "Epoch 7/10\n",
      " - 0s - loss: 151.4866 - val_loss: 112.3455\n",
      "Epoch 8/10\n",
      " - 0s - loss: 151.2309 - val_loss: 112.1787\n",
      "Epoch 9/10\n",
      " - 0s - loss: 150.9205 - val_loss: 111.6614\n",
      "Epoch 10/10\n",
      " - 0s - loss: 150.6705 - val_loss: 111.4698\n",
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 150.4133 - val_loss: 111.1513\n",
      "Epoch 2/10\n",
      " - 0s - loss: 150.1462 - val_loss: 110.4680\n",
      "Epoch 3/10\n",
      " - 0s - loss: 149.8744 - val_loss: 110.2946\n",
      "Epoch 4/10\n",
      " - 0s - loss: 149.6804 - val_loss: 109.7439\n",
      "Epoch 5/10\n",
      " - 0s - loss: 149.3738 - val_loss: 109.3867\n",
      "Epoch 6/10\n",
      " - 0s - loss: 149.1681 - val_loss: 109.0756\n",
      "Epoch 7/10\n",
      " - 0s - loss: 148.8380 - val_loss: 108.5532\n",
      "Epoch 8/10\n",
      " - 0s - loss: 148.5854 - val_loss: 108.3377\n",
      "Epoch 9/10\n",
      " - 0s - loss: 148.2876 - val_loss: 107.9512\n",
      "Epoch 10/10\n",
      " - 0s - loss: 148.0455 - val_loss: 107.9092\n",
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 147.8994 - val_loss: 107.6924\n",
      "Epoch 2/10\n",
      " - 0s - loss: 147.5585 - val_loss: 107.2439\n",
      "Epoch 3/10\n",
      " - 0s - loss: 147.2520 - val_loss: 106.8847\n",
      "Epoch 4/10\n",
      " - 0s - loss: 147.0336 - val_loss: 106.6692\n",
      "Epoch 5/10\n",
      " - 0s - loss: 146.7266 - val_loss: 105.8388\n",
      "Epoch 6/10\n",
      " - 0s - loss: 146.4747 - val_loss: 105.5741\n",
      "Epoch 7/10\n",
      " - 0s - loss: 146.2175 - val_loss: 105.2567\n",
      "Epoch 8/10\n",
      " - 0s - loss: 145.9970 - val_loss: 105.0080\n",
      "Epoch 9/10\n",
      " - 0s - loss: 145.7302 - val_loss: 104.7789\n",
      "Epoch 10/10\n",
      " - 0s - loss: 145.4134 - val_loss: 104.4623\n"
     ]
    }
   ],
   "source": [
    "#B.Normalize the data\n",
    "\n",
    "#normalize the data\n",
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "\n",
    "#Run the model on normalized data\n",
    "\n",
    "n_cols_norm_data = predictors_norm.shape[1] # number of predictors\n",
    "\n",
    "# build the model\n",
    "\n",
    "hidden_n_nodes = 10\n",
    "\n",
    "model_norm_data = regression_model(n_cols_norm_data, hidden_n_nodes)\n",
    "\n",
    "list_of_scores_norm_data = []\n",
    "\n",
    "num_runs_norm_data = 50\n",
    "for i in range (0,num_runs_norm_data):\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(predictors_norm, target, validation_split=0.3, epochs=50, verbose=2) \n",
    "\n",
    "    # evaluate the model\n",
    "    scores_norm_data = model.evaluate(predictors_norm, target, verbose=0)\n",
    "    \n",
    "    list_of_scores_norm_data.append(scores_norm_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean for norm data is:  135.85593379492897\n",
      "std_dev for norm data is:  2.8908941635035954\n"
     ]
    }
   ],
   "source": [
    "#Calculate mean and stdev for normalized data\n",
    "\n",
    "mean_score_norm_data = mean(list_of_scores_norm_data)\n",
    "\n",
    "stand_dev_norm_data = stdev(list_of_scores_norm_data)\n",
    "\n",
    "print(\"mean for norm data is: \", mean_score_norm_data)\n",
    "\n",
    "print(\"std_dev for norm data is: \", stand_dev_norm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference between the means is:  5765.804620665022\n"
     ]
    }
   ],
   "source": [
    "#How does the mean of the mean squared errors compare to that from Step A?\n",
    "mean_difference = abs(mean_score - mean_score_norm_data)\n",
    "\n",
    "print(\"difference between the means is: \", mean_difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/15\n",
      " - 0s - loss: 145.2217 - val_loss: 103.9535\n",
      "Epoch 2/15\n",
      " - 0s - loss: 144.8662 - val_loss: 104.0163\n",
      "Epoch 3/15\n",
      " - 0s - loss: 144.6664 - val_loss: 103.6305\n",
      "Epoch 4/15\n",
      " - 0s - loss: 144.3803 - val_loss: 103.1867\n",
      "Epoch 5/15\n",
      " - 0s - loss: 144.1793 - val_loss: 103.2374\n",
      "Epoch 6/15\n",
      " - 0s - loss: 143.8402 - val_loss: 102.4094\n",
      "Epoch 7/15\n",
      " - 0s - loss: 143.5820 - val_loss: 102.4149\n",
      "Epoch 8/15\n",
      " - 0s - loss: 143.3422 - val_loss: 102.0711\n",
      "Epoch 9/15\n",
      " - 0s - loss: 143.1285 - val_loss: 101.1545\n",
      "Epoch 10/15\n",
      " - 0s - loss: 142.8047 - val_loss: 100.9577\n",
      "Epoch 11/15\n",
      " - 0s - loss: 142.6113 - val_loss: 101.2343\n",
      "Epoch 12/15\n",
      " - 0s - loss: 142.4076 - val_loss: 100.6234\n",
      "Epoch 13/15\n",
      " - 0s - loss: 142.1741 - val_loss: 100.8398\n",
      "Epoch 14/15\n",
      " - 0s - loss: 141.9364 - val_loss: 100.3143\n",
      "Epoch 15/15\n",
      " - 0s - loss: 141.7604 - val_loss: 100.0987\n",
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/15\n",
      " - 0s - loss: 141.5302 - val_loss: 99.9496\n",
      "Epoch 2/15\n",
      " - 0s - loss: 141.3444 - val_loss: 99.6710\n",
      "Epoch 3/15\n",
      " - 0s - loss: 141.0112 - val_loss: 99.4930\n",
      "Epoch 4/15\n",
      " - 0s - loss: 140.8094 - val_loss: 99.1492\n",
      "Epoch 5/15\n",
      " - 0s - loss: 140.5900 - val_loss: 98.9854\n",
      "Epoch 6/15\n",
      " - 0s - loss: 140.3525 - val_loss: 98.2781\n",
      "Epoch 7/15\n",
      " - 0s - loss: 140.1302 - val_loss: 98.4267\n",
      "Epoch 8/15\n",
      " - 0s - loss: 139.9326 - val_loss: 97.9349\n",
      "Epoch 9/15\n",
      " - 0s - loss: 139.6722 - val_loss: 97.8530\n",
      "Epoch 10/15\n",
      " - 0s - loss: 139.3907 - val_loss: 97.5954\n",
      "Epoch 11/15\n",
      " - 0s - loss: 139.1236 - val_loss: 97.4675\n",
      "Epoch 12/15\n",
      " - 1s - loss: 138.8924 - val_loss: 97.5844\n",
      "Epoch 13/15\n",
      " - 0s - loss: 138.6772 - val_loss: 97.0095\n",
      "Epoch 14/15\n",
      " - 0s - loss: 138.4386 - val_loss: 96.8212\n",
      "Epoch 15/15\n",
      " - 0s - loss: 138.1955 - val_loss: 96.9203\n",
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/15\n",
      " - 0s - loss: 137.9446 - val_loss: 96.7396\n",
      "Epoch 2/15\n",
      " - 0s - loss: 137.7066 - val_loss: 96.3708\n",
      "Epoch 3/15\n",
      " - 0s - loss: 137.4577 - val_loss: 96.1028\n",
      "Epoch 4/15\n",
      " - 0s - loss: 137.2860 - val_loss: 95.8040\n",
      "Epoch 5/15\n",
      " - 0s - loss: 137.0579 - val_loss: 95.7374\n",
      "Epoch 6/15\n",
      " - 1s - loss: 136.7951 - val_loss: 95.9139\n",
      "Epoch 7/15\n",
      " - 0s - loss: 136.5949 - val_loss: 95.7313\n",
      "Epoch 8/15\n",
      " - 0s - loss: 136.3615 - val_loss: 95.6908\n",
      "Epoch 9/15\n",
      " - 0s - loss: 136.2155 - val_loss: 95.4874\n",
      "Epoch 10/15\n",
      " - 0s - loss: 135.9007 - val_loss: 95.3558\n",
      "Epoch 11/15\n",
      " - 0s - loss: 135.6876 - val_loss: 95.2022\n",
      "Epoch 12/15\n",
      " - 0s - loss: 135.4149 - val_loss: 95.2205\n",
      "Epoch 13/15\n",
      " - 0s - loss: 135.1607 - val_loss: 94.8302\n",
      "Epoch 14/15\n",
      " - 0s - loss: 134.9565 - val_loss: 94.7183\n",
      "Epoch 15/15\n",
      " - 0s - loss: 134.7209 - val_loss: 94.6354\n"
     ]
    }
   ],
   "source": [
    "#C. Increase the number of epochs to 100\n",
    "\n",
    "list_of_scores_norm_data_100_epochs = []\n",
    "\n",
    "num_runs_norm_data = 50\n",
    "for i in range (0,num_runs_norm_data):\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(predictors_norm, target, validation_split=0.3, epochs=100, verbose=2) \n",
    "\n",
    "    # evaluate the model\n",
    "    scores_norm_data = model.evaluate(predictors_norm, target, verbose=0)\n",
    "    \n",
    "    list_of_scores_norm_data_100_epochs.append(scores_norm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean for norm data is:  125.7555475821387\n",
      "std_dev for norm data is:  3.295232395793573\n"
     ]
    }
   ],
   "source": [
    "#Calculate mean and stdev for normalized data\n",
    "\n",
    "mean_score_norm_data_100_epochs = mean(list_of_scores_norm_data_100_epochs)\n",
    "\n",
    "stand_dev_norm_data_100_epochs = stdev(list_of_scores_norm_data_100_epochs)\n",
    "\n",
    "print(\"mean for norm data is: \", mean_score_norm_data_100_epochs)\n",
    "\n",
    "print(\"std_dev for norm data is: \", stand_dev_norm_data_100_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference between the means is:  10.10038621279027\n"
     ]
    }
   ],
   "source": [
    "#How does the mean of the mean squared errors compare to that from Step A?\n",
    "mean_difference_100_vs_50 = abs(mean_score_norm_data - mean_score_norm_data_100_epochs)\n",
    "\n",
    "print(\"difference between the means is: \", mean_difference_100_vs_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D.Increase the number of hidden layers\n",
    "\n",
    "#define new regression model\n",
    "def regression_model_3_layers(n_cols, hidden_n_nodes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_n_nodes, activation='relu', input_shape=(n_cols,))) #hidden layer 1\n",
    "    model.add(Dense(hidden_n_nodes, activation='relu')) #hidden layer 2\n",
    "    model.add(Dense(hidden_n_nodes, activation='relu')) #hidden layer 3\n",
    "    model.add(Dense(1)) # output layer\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1661.6956 - val_loss: 1191.6456\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1625.4521 - val_loss: 1156.3296\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1568.3707 - val_loss: 1102.4738\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1477.8085 - val_loss: 1022.8016\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1341.1950 - val_loss: 911.5466\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1150.3011 - val_loss: 764.1497\n",
      "Epoch 7/10\n",
      " - 0s - loss: 908.0839 - val_loss: 593.9590\n",
      "Epoch 8/10\n",
      " - 0s - loss: 651.0134 - val_loss: 446.1498\n",
      "Epoch 9/10\n",
      " - 0s - loss: 447.6972 - val_loss: 354.1951\n",
      "Epoch 10/10\n",
      " - 0s - loss: 330.9708 - val_loss: 307.3113\n",
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 275.8790 - val_loss: 282.6341\n",
      "Epoch 2/10\n",
      " - 0s - loss: 253.3028 - val_loss: 261.9779\n",
      "Epoch 3/10\n",
      " - 0s - loss: 236.6322 - val_loss: 246.3339\n",
      "Epoch 4/10\n",
      " - 0s - loss: 226.2760 - val_loss: 233.5411\n",
      "Epoch 5/10\n",
      " - 1s - loss: 217.4230 - val_loss: 223.2724\n",
      "Epoch 6/10\n",
      " - 0s - loss: 209.8948 - val_loss: 215.1878\n",
      "Epoch 7/10\n",
      " - 0s - loss: 203.9304 - val_loss: 207.9949\n",
      "Epoch 8/10\n",
      " - 0s - loss: 197.7920 - val_loss: 201.0667\n",
      "Epoch 9/10\n",
      " - 0s - loss: 192.2902 - val_loss: 195.2468\n",
      "Epoch 10/10\n",
      " - 0s - loss: 187.6024 - val_loss: 190.3536\n",
      "Train on 721 samples, validate on 309 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 182.8999 - val_loss: 187.2749\n",
      "Epoch 2/10\n",
      " - 0s - loss: 178.7187 - val_loss: 182.7630\n",
      "Epoch 3/10\n",
      " - 0s - loss: 174.9935 - val_loss: 179.2113\n",
      "Epoch 4/10\n",
      " - 0s - loss: 171.3799 - val_loss: 175.9404\n",
      "Epoch 5/10\n",
      " - 0s - loss: 168.8607 - val_loss: 172.0602\n",
      "Epoch 6/10\n",
      " - 0s - loss: 165.2404 - val_loss: 170.0486\n",
      "Epoch 7/10\n",
      " - 0s - loss: 162.3816 - val_loss: 167.3638\n",
      "Epoch 8/10\n",
      " - 0s - loss: 159.8868 - val_loss: 165.9702\n",
      "Epoch 9/10\n",
      " - 0s - loss: 157.2136 - val_loss: 162.7450\n",
      "Epoch 10/10\n",
      " - 0s - loss: 154.7671 - val_loss: 160.3391\n"
     ]
    }
   ],
   "source": [
    "# build the new model\n",
    "\n",
    "hidden_n_nodes_3l = 10\n",
    "\n",
    "model_norm_data_3l = regression_model_3_layers(n_cols_norm_data, hidden_n_nodes_3l)\n",
    "\n",
    "list_of_scores_norm_data_3l = []\n",
    "\n",
    "num_runs_norm_data_3l = 50\n",
    "for i in range (0,num_runs_norm_data_3l):\n",
    "    \n",
    "    # fit the model\n",
    "    model_norm_data_3l.fit(predictors_norm, target, validation_split=0.3, epochs=50, verbose=2) \n",
    "\n",
    "    # evaluate the model\n",
    "    scores_norm_data_3l = model_norm_data_3l.evaluate(predictors_norm, target, verbose=0)\n",
    "    \n",
    "    list_of_scores_norm_data_3l.append(scores_norm_data_3l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean for norm data for new model with 3 hidden layers is:  212.80744892601828\n",
      "std_dev for norm data for new model with 3 hidden layers is:  74.52893544869467\n"
     ]
    }
   ],
   "source": [
    "#Calculate mean and stdev for normalized data for new model\n",
    "\n",
    "mean_score_norm_data_3l = mean(list_of_scores_norm_data_3l)\n",
    "\n",
    "stand_dev_norm_data_3l = stdev(list_of_scores_norm_data_3l)\n",
    "\n",
    "print(\"mean for norm data for new model with 3 hidden layers is: \", mean_score_norm_data_3l)\n",
    "\n",
    "print(\"std_dev for norm data for new model with 3 hidden layers is: \", stand_dev_norm_data_3l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference between the means is:  76.95151513108931\n"
     ]
    }
   ],
   "source": [
    "#How does the mean of the mean squared errors compare to that from Step b?\n",
    "mean_difference_3_layers = abs(mean_score_norm_data_3l - mean_score_norm_data)\n",
    "\n",
    "print(\"difference between the means is: \", mean_difference_3_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
